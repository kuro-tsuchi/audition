# 1. RocketMQ

## 1.1. RocketMQ 是什么？

RocketMQ 是一个队列模型的消息中间件，具有高性能，高可靠，高实时，分布式的特点。它是一个采用 Java 语言开发的分布式的消息系统，由阿里巴巴团队开发，在 2016 年底贡献给 Apache

### 1.1.1. RocketMQ 中的消息模型

> 主题模型/发布订阅模型就是一个标准，中间件照着这个标准去实现。每个消息中间件的底层设计都是不一样的，就比如 Kafka 中的分区 , RocketMQ 中的队列 , RabbitMQ 中的 Exchange .

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef383d3e8c9788.jpg)

生产者组中的生产者会向主题发送消息，而主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。

1. Producer Group 生产者组：代表某一类的生产者，比如有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。
1. Consumer Group 消费者组：代表某一类的消费者，比如有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。
1. Topic 主题：代表一类消息，比如订单消息，物流消息等等。

每个主题中都有多个队列 (分布在不同的 Broker 中，如果是集群的话，Broker 又分布在不同的服务器中), 集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。所以一般要控制消费者组中的消费者个数和主题中队列个数相同 .

### 1.1.2. 每个消费组在每个队列上维护一个消费位置 , 为什么呢？

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3850c808d707.jpg)

因为刚刚画的仅仅是一个消费者组，知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的 (因为其它消费者组也需要呀), 它仅仅是为每个消费者组维护一个消费位移 (offset) , 每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3857fefaa079.jpg)

### 1.1.3. 为什么一个主题中需要维护多个队列 ?

是为了提高并发能力

如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到发布订阅模式 .如下图。

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38600cdb6d4b.jpg)

但是，这样生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 Consumer 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。

所以总结来说，RocketMQ 通过使用在一个 Topic 中配置多个队列并且每个队列维护每个消费者组的消费位置，实现了主题模式/发布订阅模式 .

## 1.2. RocketMQ 的架构图

RocketMQ 技术架构中有四大角色 NameServer , Broker , Producer , Consumer

- Broker: 主要负责消息的存储，投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker , 消费者从 Broker 拉取消息并消费。

  这里，还得普及一下关于 Broker , Topic 和 队列的关系。上面讲解了 Topic 和队列的关系——一个 Topic 中存在多个队列，那么这个 Topic 和队列存放在哪呢？

一个 Topic 分布在多个 Broker 上，一个 Broker 可以配置多个 Topic , 它们是多对多的关系。

如果某个 Topic 消息量很大，应该给它多配置几个队列 (上文中提到了提高并发能力), 并且尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 .

Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38687488a5a4.jpg)

> 所以说需要配置多个 Broker.

- NameServer: 不知道你们有没有接触过 ZooKeeper 和 Spring Cloud 中的 Eureka , 它其实也是一个注册中心 , 主要提供两个功能:Broker 管理 和路由信息管理 .说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息 (Broker 的路由表), 消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信 (生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息).

- Producer: 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。

- Consumer: 消息消费的角色，支持分布式集群方式部署。支持以 push 推，pull 拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。

听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef386c6d1e8bdb.jpg)

嗯？你可能会发现一个问题，这老家伙 NameServer 干啥用的，这不多余吗？直接 Producer , Consumer 和 Broker 直接进行生产消息，消费消息不就好了么？

但是，上文提到过 Broker 是需要保证高可用的，如果整个系统仅仅靠着一个 Broker 来维持的话，那么这个 Broker 的压力会不会很大？所以需要使用多个 Broker 来保证负载均衡 .

如果说，的消费者和生产者直接和多个 Broker 相连，那么当 Broker 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 NameServer 注册中心就是用来解决这个问题的。

> 如果还不是很理解的话，可以去看介绍 Spring Cloud 的那篇文章，其中介绍了 Eureka 注册中心。

当然，RocketMQ 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。给出一张官网的架构图，大家尝试理解一下。

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef386fa3be1e53.jpg)

其实和最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听细细道来 🤨.

第一，的 Broker 做了集群并且还进行了主从部署 , 由于消息分布在各个 Broker 上，一旦某个 Broker 宕机，则该 Broker 上的消息读写都会受到影响。所以 Rocketmq 提供了 master/slave 的结构，salve 定时从 master 同步数据 (同步刷盘或者异步刷盘), 如果 master 宕机，则 slave 提供消费服务，但是不能写入消息 (后面还会提到哦).

第二，为了保证 HA , 的 NameServer 也做了集群部署，但是请注意它是去中心化 的。也就意味着它没有主节点，你可以很明显地看出 NameServer 的所有节点是没有进行 Info Replicate 的，在 RocketMQ 中是通过单个 Broker 和所有 NameServer 保持长连接 , 并且在每隔 30 秒 Broker 会向所有 Nameserver 发送心跳，心跳包含了自身的 Topic 配置信息，这个步骤就对应这上面的 Routing Info .

第三，在生产者需要向 Broker 发送消息的时候，需要先从 NameServer 获取关于 Broker 的路由信息，然后通过轮询 的方法去向每个队列中生产数据以达到负载均衡 的效果。

第四，消费者通过 NameServer 获取所有 Broker 的路由信息后，向 Broker 发送 Pull 请求来获取消息数据.Consumer 可以以两种模式启动——广播 (Broadcast) 和集群 (Cluster).广播模式下，一条消息会发送给同一个消费组中的所有消费者 , 集群模式下消息只会发送给一个消费者。

## 1.3. 如何解决顺序消费，重复消费

> Kafka 的架构基本和 RocketMQ 类似，只是它注册中心使用了 Zookeeper , 它的分区就相当于 RocketMQ 中的队列

### 1.3.1. 顺序消费

RocketMQ 在主题上是无序的，它只有在队列层面才是保证有序的。

Producer 生产消息的时候会进行轮询 (取决你的负载均衡策略) 来向同一主题的不同消息队列发送消息。那么如果此时有几个消息分别是同一个订单的创建，支付，发货，在轮询的策略下这三个消息会被发送到不同队列 , 因为在不同的队列此时就无法使用 RocketMQ 带来的队列有序特性来保证消息有序性了。

> 轮询是按照某种算法进行顺序触发，轮询时会保存当前执行后的索引，以便于下次执行时可以拿到开始索引位置，以达到负载均衡的目的。

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3874585e096e.jpg)

#### 1.3.1.1. rocketmq 实现顺序消费

> 顺序消息在日常的功能场景中很常见，比如点外卖生成外卖订单，付款，送餐的消息需要保证严格的顺序。

消息队列为了保证高并发，一般会配置多个分区，并支持多个客户端同时消费，因此只能保证分区内有序。

要保证一个订单有关的消息顺序消费，有两点需要注意，一是将订单有关的消息发送到相关 Topic 中同一个 queue 里，二是消费者按照先进先出的原则进行消费。

1. 全局顺序消息：
   RocketMQ 如果要保证全局顺序消息的话需要在生产端只保留一个读写队列，然后消费端只有一个消费线程，这样会降低 RocketMQ 的高可用和高吞吐量。

1. 分区顺序消息：
   分区顺序消息同样需要生产端和消费端配合，生产端根据同一个订单 ID 把消息路由到同一个 MessageQueue，消费端控制从同一个 MessageQueue 取出的消息不被并发处理。

### 1.3.2. 重复消费 (幂等)

幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。也就是对同一个消息的处理结果，执行多少次都不变。

比如说，这个时候有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500.但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动 (当然还有很多种情况，比如 Broker 意外重启等等), 这条回应没有发送成功。那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？

#### 1.3.2.1. 那么如何给业务实现幂等呢？

只要处理消息之前先根据业务判断一下本次操作是否已经执行过了，如果已经执行过了，那就不再执行了，这样就可以保证消费者的幂等性。

比如每条消息都会有一条唯一的消息 msgId，消费者接收到消息会存储消息日志，如果日志中存在相同 ID 的消息，就证明这条消息已经被处理过了。可以使用写入 Redis 来保证，因为 Redis 的 key 和 value 就是天然支持幂等的。当然还有使用数据库插入法 , 基于数据库的唯一键来保证重复数据不会被插入多条。

## 1.4. rocketmq 如何保证消息不丢失

![](<https://img-blog.csdnimg.cn/20200714172547692.png?x-oss-process=image/watermark>, type_ZmFuZ3poZW5naGVpdGk, shadow_10, text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nMTgyMzgwMzI4OTE=, size_16, color_FFFFFF, t_70)

1. 生产阶段，Producer 新建消息，然后通过网络将消息投递给 MQ Broker
1. 存储阶段，消息将会存储在 Broker 端磁盘中
1. 消息阶段，Consumer 将会从 Broker 拉取消息

### 1.4.1. 从 Producer 分析：如何确保消息正确的发送到了 Broker?

1. 默认情况下，可以通过同步的方式阻塞式的发送，check SendStatus，状态是 OK，表示消息一定成功的投递到了 Broker，状态超时或者失败，则会触发默认的 2 次重试。此方法的发送结果，可能 Broker 存储成功了，也可能没成功
1. 采取事务消息的投递方式，并不能保证消息 100% 投递成功到了 Broker，但是如果消息发送 Ack 失败的话，此消息会存储在 CommitLog 当中，但是对 ConsumerQueue 是不可见的。可以在日志中查看到这条异常的消息，严格意义上来讲，也并没有完全丢失
1. RocketMQ 支持 日志的索引，如果一条消息发送之后超时，也可以通过查询日志的 API，来 check 是否在 Broker 存储成功

### 1.4.2. 从 Broker 分析：如果确保接收到的消息不会丢失？

1. 消息支持持久化到 Commitlog 里面，即使宕机后重启，未消费的消息也是可以加载出来的
1. Broker 自身支持同步刷盘，异步刷盘的策略，可以保证接收到的消息一定存储在本地的内存中
1. Broker 集群支持 1 主 N 从的策略，支持同步复制和异步复制的方式，同步复制可以保证即使 Master 磁盘崩溃，消息仍然不会丢失

### 1.4.3. 从 Cunmser 分析：如何确保拉取到的消息被成功消费？

1. 消费者可以根据自身的策略批量 Pull 消息
1. Consumer 自身维护一个持久化的 offset ,  标记已经成功消费或者已经成功发回到 broker 的消息下标
1. 如果 Consumer 消费失败，那么它会把这个消息发回给 Broker，发回成功后，再更新自己的 offset
1. 如果 Consumer 消费失败，发回给 broker 时，broker 挂掉了，那么 Consumer 会定时重试这个操作
1. 如果 Consumer 和 broker 一起挂了，消息也不会丢失，因为 consumer 里面的 offset 是定时持久化的，重启之后，继续拉取 offset 之前的消息到本地

## 1.5. 消息堆积问题 (削峰)

产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。

当流量到峰值的时候是因为生产者生产太快，可以使用一些限流降级的方法，当然也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。

如果消费者消费过慢的话，可以先检查是否是消费者出现了大量的消费错误，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。

### 1.5.1. 解决消息堆积问题的方法

当然，最快速解决消息堆积问题的方法还是增加消费者实例，同时需要增加每个主题的队列数量，在 RocketMQ 中，一个队列只会被一个消费者消费

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef387d939ab66d.jpg)

## 1.6. 回溯消费

回溯消费是指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，在 RocketMQ 中，Broker 在向 Consumer 投递成功消息后，消息仍然需要保留。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费 1 小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度.RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。

## 1.7. RocketMQ 的刷盘机制

RocketMQ 需要将消息存储到磁盘上，这样才能保证断电后消息不会丢失。同时这样才可以让存储的消息量可以超出内存的限制. RocketMQ 为了提高性能，会尽量保证磁盘的顺序写。消息在写入磁盘时，有两种写磁盘的方式，同步刷盘和异步刷盘

### 1.7.1. 同步刷盘和异步刷盘

![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef387fba311cda.jpg)

如上图所示，在同步刷盘中需要等待一个刷盘成功的 ACK , 同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是性能上会有较大影响 , 一般地适用于金融等特定业务场景。

而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行，降低了读写延迟 , 提高了 MQ 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。

一般地，异步刷盘只有在 Broker 意外宕机的时候会丢失部分数据，你可以设置 Broker 的参数 FlushDiskType 来调整你的刷盘策略 (ASYNC_FLUSH 或者 SYNC_FLUSH).

### 1.7.2. 同步复制和异步复制

同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 Borker 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。

1. 同步复制：也叫同步双写，也就是说，只有消息同步双写到主从结点上时才返回写入成功 .
1. 异步复制：消息写入主节点之后就直接返回写入成功 .
